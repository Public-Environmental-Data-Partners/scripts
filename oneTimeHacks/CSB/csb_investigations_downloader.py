#!/usr/bin/env python3
"""
CSB Investigations Downloader

This script downloads investigation files and saves landing pages as PDFs.

DEPENDENCIES:
- Requires CSV file generated by csb_investigations_scraper.py
- Expected CSV columns: id, source_url, incident_title, location, category,
  date_incident, date_final_report, description, download_url, download_title, file_type

WHAT IT DOES:
1. Downloads all files from the download_url column
2. Names files using their native filename (if unique) or investigation_id + _NJY# suffix for duplicates
3. Saves a PDF of each investigation landing page (source_url)

REQUIREMENTS:
- requests: pip install requests
- selenium (optional, for PDF generation): pip install selenium
- Firefox browser and geckodriver (for PDF generation)
"""

import csv
import os
import sys
import logging
import requests
import time
from collections import defaultdict
from pathlib import Path
from urllib.parse import urlparse, unquote
import argparse

# Try to import selenium for PDF generation
try:
    from selenium import webdriver
    from selenium.webdriver.firefox.options import Options
    from selenium.webdriver.firefox.service import Service
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.common.by import By
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("Warning: selenium not installed. Landing page PDF generation will be skipped.")
    print("To enable PDF generation, install selenium: pip install selenium")


class CSBInvestigationsDownloader:
    def __init__(self, csv_path, output_dir="downloads"):
        self.csv_path = csv_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # Track filenames to detect duplicates
        self.filename_counts = defaultdict(int)
        self.downloaded_urls = set()

        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler(self.output_dir / 'download_log.txt')
            ]
        )

    def get_filename_from_url(self, url):
        """Extract filename from URL"""
        parsed = urlparse(url)
        filename = unquote(os.path.basename(parsed.path))
        return filename

    def get_unique_filename(self, base_filename, investigation_id):
        """
        Generate a unique filename with investigation ID prepended.
        Format: [investigation_id]_[filename] or [investigation_id]_NJY#_[filename] for duplicates
        """
        # Clean the filename
        base_filename = base_filename.strip()
        if not base_filename:
            base_filename = "document.pdf"

        # Create the new filename with ID prefix
        new_filename = f"{investigation_id}_{base_filename}"

        # Check if we've seen this exact new filename before
        self.filename_counts[new_filename] += 1

        if self.filename_counts[new_filename] == 1:
            # First occurrence - use the ID-prefixed name
            return new_filename
        else:
            # Duplicate - add NJY# suffix
            name, ext = os.path.splitext(base_filename)
            suffix_num = self.filename_counts[new_filename] - 1
            return f"{investigation_id}_NJY{suffix_num}_{name}{ext}"

    def download_file(self, url, output_path, retries=3):
        """Download a file with retry logic"""
        for attempt in range(retries):
            try:
                logging.info(f"Downloading: {url}")
                response = requests.get(url, timeout=30, stream=True)
                response.raise_for_status()

                with open(output_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)

                logging.info(f"Saved to: {output_path}")
                return True

            except Exception as e:
                logging.warning(f"Attempt {attempt + 1} failed: {e}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)  # Exponential backoff
                else:
                    logging.error(f"Failed to download {url}: {e}")
                    return False

    def save_landing_page_as_pdf(self, url, output_path):
        """Save a landing page as PDF using Selenium with Firefox"""
        if not SELENIUM_AVAILABLE:
            return False

        driver = None
        try:
            logging.info(f"Generating PDF for landing page: {url}")

            # Setup Firefox options for headless PDF generation
            firefox_options = Options()
            firefox_options.add_argument('--headless')

            # Set PDF print preferences for Firefox
            firefox_options.set_preference('print.always_print_silent', True)
            firefox_options.set_preference('print.printer_Mozilla_Save_to_PDF.print_to_file', True)
            firefox_options.set_preference('print_printer', 'Mozilla Save to PDF')
            firefox_options.set_preference('print.save_as_pdf.links.enabled', True)

            # Set download directory
            firefox_options.set_preference('browser.download.folderList', 2)
            firefox_options.set_preference('browser.download.dir', str(self.output_dir.absolute()))
            firefox_options.set_preference('browser.download.useDownloadDir', True)
            firefox_options.set_preference('browser.helperApps.neverAsk.saveToDisk', 'application/pdf')

            # Additional print settings
            firefox_options.set_preference('print.print_footerleft', '')
            firefox_options.set_preference('print.print_footerright', '')
            firefox_options.set_preference('print.print_headerleft', '')
            firefox_options.set_preference('print.print_headerright', '')

            driver = webdriver.Firefox(options=firefox_options)
            driver.get(url)

            # Wait for page to load
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            time.sleep(2)  # Additional wait for dynamic content

            # Use Firefox's native print() method
            # This will use the configured print preferences
            pdf_base64 = driver.print_page()

            # Save the PDF
            import base64
            with open(output_path, 'wb') as f:
                f.write(base64.b64decode(pdf_base64))

            logging.info(f"Saved landing page PDF to: {output_path}")
            return True

        except AttributeError:
            # print_page() method not available in older selenium versions
            logging.error(f"print_page() method not available. Please upgrade selenium: pip install --upgrade selenium")
            logging.info(f"Skipping landing page PDF for: {url}")
            return False
        except Exception as e:
            logging.error(f"Failed to generate PDF for {url}: {e}")
            return False
        finally:
            if driver:
                driver.quit()

    def process_csv(self):
        """Process the CSV file and download all files"""
        logging.info(f"Starting download process from: {self.csv_path}")
        logging.info(f"Output directory: {self.output_dir}")

        # Track landing pages we've already processed
        landing_pages_processed = set()

        stats = {
            'total_rows': 0,
            'downloads_successful': 0,
            'downloads_failed': 0,
            'downloads_skipped': 0,
            'landing_pages_saved': 0,
            'landing_pages_failed': 0
        }

        with open(self.csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)

            for row in reader:
                stats['total_rows'] += 1

                investigation_id = row.get('id', '').strip()
                download_url = row.get('download_url', '').strip()
                source_url = row.get('source_url', '').strip()
                date_incident = row.get('date_incident', '').strip()

                # Fallback: if no investigation ID, use date_incident in MMDDYYYY format
                if not investigation_id:
                    if date_incident:
                        # Try to parse the date and reformat it
                        try:
                            from datetime import datetime
                            # Try common date formats
                            for fmt in ['%m/%d/%Y', '%Y-%m-%d', '%m-%d-%Y', '%d/%m/%Y']:
                                try:
                                    date_obj = datetime.strptime(date_incident, fmt)
                                    investigation_id = date_obj.strftime('%m%d%Y')
                                    logging.info(f"Row {stats['total_rows']}: Using date as ID: {investigation_id}")
                                    break
                                except ValueError:
                                    continue

                            if not investigation_id:
                                # If parsing failed, try to clean and use the date as-is
                                investigation_id = date_incident.replace('/', '').replace('-', '')
                                logging.warning(f"Row {stats['total_rows']}: Could not parse date, using cleaned version: {investigation_id}")
                        except Exception as e:
                            logging.warning(f"Row {stats['total_rows']}: Error parsing date: {e}")

                    if not investigation_id:
                        logging.warning(f"Row {stats['total_rows']}: Missing investigation ID and date, skipping")
                        continue

                # Download the file if URL exists
                if download_url:
                    # Get native filename from URL
                    native_filename = self.get_filename_from_url(download_url)

                    # Generate unique filename
                    unique_filename = self.get_unique_filename(native_filename, investigation_id)
                    output_path = self.output_dir / unique_filename

                    # Check if file already exists on disk
                    if output_path.exists():
                        logging.info(f"File already exists, skipping: {output_path.name}")
                        stats['downloads_skipped'] += 1
                        self.downloaded_urls.add(download_url)
                    # Skip if we've already downloaded this URL in this session
                    elif download_url in self.downloaded_urls:
                        logging.info(f"Already downloaded in this session: {download_url}")
                        stats['downloads_skipped'] += 1
                    else:
                        # Download the file
                        if self.download_file(download_url, output_path):
                            stats['downloads_successful'] += 1
                            self.downloaded_urls.add(download_url)
                        else:
                            stats['downloads_failed'] += 1

                # Save landing page as PDF (once per investigation)
                if source_url and source_url not in landing_pages_processed:
                    landing_page_filename = f"{investigation_id}_landing_page.pdf"
                    landing_page_path = self.output_dir / landing_page_filename

                    # Check if landing page PDF already exists
                    if landing_page_path.exists():
                        logging.info(f"Landing page PDF already exists, skipping: {landing_page_path.name}")
                        landing_pages_processed.add(source_url)
                    else:
                        if self.save_landing_page_as_pdf(source_url, landing_page_path):
                            stats['landing_pages_saved'] += 1
                        else:
                            stats['landing_pages_failed'] += 1

                        landing_pages_processed.add(source_url)

        # Print summary
        logging.info("\n" + "="*60)
        logging.info("DOWNLOAD SUMMARY")
        logging.info("="*60)
        logging.info(f"Total rows processed: {stats['total_rows']}")
        logging.info(f"Downloads successful: {stats['downloads_successful']}")
        logging.info(f"Downloads failed: {stats['downloads_failed']}")
        logging.info(f"Downloads skipped (duplicates): {stats['downloads_skipped']}")
        logging.info(f"Landing pages saved: {stats['landing_pages_saved']}")
        logging.info(f"Landing pages failed: {stats['landing_pages_failed']}")
        logging.info("="*60)

        return stats


def main():
    parser = argparse.ArgumentParser(
        description='Download CSB investigation files and save landing pages as PDFs'
    )
    parser.add_argument(
        '--csv',
        default='csb_past_investigations.csv',
        help='Path to the investigations CSV file (default: csb_past_investigations.csv)'
    )
    parser.add_argument(
        '--output',
        default='csb_investigations_downloads',
        help='Output directory for downloads (default: csb_investigations_downloads)'
    )

    args = parser.parse_args()

    # Check if CSV exists
    if not os.path.exists(args.csv):
        print(f"Error: CSV file not found: {args.csv}")
        sys.exit(1)

    # Create downloader and process
    downloader = CSBInvestigationsDownloader(args.csv, args.output)
    downloader.process_csv()


if __name__ == '__main__':
    main()
