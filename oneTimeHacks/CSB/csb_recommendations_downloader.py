#!/usr/bin/env python3
"""
CSB Recommendations Downloader

This script downloads recommendation files from CSB.

DEPENDENCIES:
- Requires CSV file generated by csb_recommendations_scraper.py
- Expected CSV columns: root_id, file_id, recommendation_id, case, recipient,
  status, recommendation_text, download_url

WHAT IT DOES:
1. Downloads all files from the download_url column
2. Names files as: [root_id]_[original_filename]
3. Skips files that already exist on disk

REQUIREMENTS:
- requests: pip install requests
"""

import csv
import os
import sys
import logging
import requests
import time
from collections import defaultdict
from pathlib import Path
from urllib.parse import urlparse, unquote
import argparse


class CSBRecommendationsDownloader:
    def __init__(self, csv_path, output_dir="downloads"):
        self.csv_path = csv_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # Track downloaded URLs
        self.downloaded_urls = set()
        self.downloaded_files = set()

        # Track duplicates
        self.duplicate_log = []

        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler(self.output_dir / 'download_log.txt')
            ]
        )

    def get_filename_from_url(self, url):
        """Extract filename from URL"""
        parsed = urlparse(url)
        filename = unquote(os.path.basename(parsed.path))
        return filename

    def create_filename(self, root_id, original_filename):
        """
        Create filename with format: [root_id]_[original_filename]
        """
        # Clean the root_id and filename
        root_id = root_id.strip()
        original_filename = original_filename.strip()

        if not original_filename:
            original_filename = "document.pdf"

        # Create the new filename
        new_filename = f"{root_id}_{original_filename}"
        return new_filename

    def download_file(self, url, output_path, retries=3):
        """Download a file with retry logic"""
        for attempt in range(retries):
            try:
                logging.info(f"Downloading: {url}")
                response = requests.get(url, timeout=30, stream=True)
                response.raise_for_status()

                with open(output_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)

                logging.info(f"Saved to: {output_path}")
                return True

            except Exception as e:
                logging.warning(f"Attempt {attempt + 1} failed: {e}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)  # Exponential backoff
                else:
                    logging.error(f"Failed to download {url}: {e}")
                    return False

    def process_csv(self):
        """Process the CSV file and download all files"""
        logging.info(f"Starting download process from: {self.csv_path}")
        logging.info(f"Output directory: {self.output_dir}")

        stats = {
            'total_rows': 0,
            'downloads_successful': 0,
            'downloads_failed': 0,
            'downloads_skipped': 0,
            'missing_data': 0
        }

        with open(self.csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)

            for row in reader:
                stats['total_rows'] += 1

                root_id = row.get('root_id', '').strip()
                download_url = row.get('download_url', '').strip()

                # Check if we have required data
                if not root_id:
                    logging.warning(f"Row {stats['total_rows']}: Missing root_id, skipping")
                    stats['missing_data'] += 1
                    continue

                if not download_url:
                    logging.debug(f"Row {stats['total_rows']}: No download URL for {root_id}")
                    stats['missing_data'] += 1
                    continue

                # Get original filename from URL
                original_filename = self.get_filename_from_url(download_url)

                # Create new filename with root_id prefix
                new_filename = self.create_filename(root_id, original_filename)
                output_path = self.output_dir / new_filename

                # Check if file already exists on disk
                if output_path.exists():
                    logging.info(f"File already exists, skipping: {new_filename}")
                    stats['downloads_skipped'] += 1
                    self.downloaded_urls.add(download_url)
                    self.downloaded_files.add(new_filename)
                # Skip if we've already downloaded this URL in this session
                elif download_url in self.downloaded_urls:
                    logging.info(f"Already downloaded in this session: {download_url}")
                    stats['downloads_skipped'] += 1
                # Skip if we've already downloaded a file with this name
                elif new_filename in self.downloaded_files:
                    logging.warning(f"DUPLICATE FILENAME: {new_filename} - URL: {download_url}")
                    stats['downloads_skipped'] += 1
                    # Log this duplicate for the report
                    self.duplicate_log.append({
                        'filename': new_filename,
                        'root_id': root_id,
                        'url': download_url,
                        'row': stats['total_rows']
                    })
                else:
                    # Download the file
                    if self.download_file(download_url, output_path):
                        stats['downloads_successful'] += 1
                        self.downloaded_urls.add(download_url)
                        self.downloaded_files.add(new_filename)
                    else:
                        stats['downloads_failed'] += 1

        # Print summary
        logging.info("\n" + "="*60)
        logging.info("DOWNLOAD SUMMARY")
        logging.info("="*60)
        logging.info(f"Total rows processed: {stats['total_rows']}")
        logging.info(f"Downloads successful: {stats['downloads_successful']}")
        logging.info(f"Downloads failed: {stats['downloads_failed']}")
        logging.info(f"Downloads skipped (already exist): {stats['downloads_skipped']}")
        logging.info(f"Rows with missing data: {stats['missing_data']}")
        logging.info(f"Duplicate filenames detected: {len(self.duplicate_log)}")
        logging.info("="*60)

        # Write duplicate log if there are any duplicates
        if self.duplicate_log:
            duplicate_log_path = self.output_dir / 'duplicates_recommendations.log'
            with open(duplicate_log_path, 'w', encoding='utf-8') as f:
                f.write("DUPLICATE FILENAMES LOG\n")
                f.write("="*60 + "\n")
                f.write(f"Total duplicates found: {len(self.duplicate_log)}\n")
                f.write("="*60 + "\n\n")

                for i, dup in enumerate(self.duplicate_log, 1):
                    f.write(f"Duplicate #{i}:\n")
                    f.write(f"  Row Number: {dup['row']}\n")
                    f.write(f"  Filename: {dup['filename']}\n")
                    f.write(f"  Root ID: {dup['root_id']}\n")
                    f.write(f"  URL: {dup['url']}\n")
                    f.write("\n")

            logging.info(f"Duplicate filenames log saved to: {duplicate_log_path}")

        return stats


def main():
    parser = argparse.ArgumentParser(
        description='Download CSB recommendation files'
    )
    parser.add_argument(
        '--csv',
        default='csb_recommendations.csv',
        help='Path to the recommendations CSV file (default: csb_recommendations.csv)'
    )
    parser.add_argument(
        '--output',
        default='csb_recommendations_downloads',
        help='Output directory for downloads (default: csb_recommendations_downloads)'
    )

    args = parser.parse_args()

    # Check if CSV exists
    if not os.path.exists(args.csv):
        print(f"Error: CSV file not found: {args.csv}")
        sys.exit(1)

    # Create downloader and process
    downloader = CSBRecommendationsDownloader(args.csv, args.output)
    downloader.process_csv()


if __name__ == '__main__':
    main()